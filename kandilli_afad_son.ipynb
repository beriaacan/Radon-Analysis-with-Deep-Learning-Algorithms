{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pwAJIKBnyGpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0b9e3f-087d-4803-dcdb-451f49d52cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDERNAME ='bitirme/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
      ],
      "metadata": {
        "id": "_arLCmU8yT1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install schedule"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9xVSITsomW8",
        "outputId": "c545024e-a496-446b-9c48-54d6867bd6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting schedule\n",
            "  Downloading schedule-1.2.1-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import schedule\n",
        "import threading\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen, Request\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "id": "PPuzS1JPoPVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "afadData = []\n",
        "kandilliData = []"
      ],
      "metadata": {
        "id": "tG-72GeXoSCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getAfadData():\n",
        "    start_date_str = \"2023-08-23T00:00:00.000Z\"\n",
        "    try:\n",
        "        array = []\n",
        "        req = Request('https://deprem.afad.gov.tr/EventData/GetEventsByFilter', method='POST')\n",
        "        req.add_header('Content-Type', 'application/json; charset=utf-8')\n",
        "\n",
        "        current_date = datetime.now()\n",
        "        end_date_str = current_date.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "\n",
        "        data = json.dumps({\n",
        "            \"EventSearchFilterList\": [\n",
        "                {\"FilterType\": 9, \"Value\": end_date_str},\n",
        "                {\"FilterType\": 8, \"Value\": start_date_str}\n",
        "            ],\n",
        "            \"Skip\": 0,\n",
        "            \"Take\": 250000,  # Remove or set to a large value to retrieve all results\n",
        "            \"SortDescriptor\": {\"field\": \"eventDate\", \"dir\": \"desc\"}\n",
        "        })\n",
        "\n",
        "        data = data.encode()\n",
        "        content = urlopen(req, data=data).read()\n",
        "        content = json.loads(content)\n",
        "\n",
        "        for i in range(len(content[\"eventList\"])):\n",
        "            el = content[\"eventList\"][i]\n",
        "            magnitudeType = el[\"magnitudeType\"]\n",
        "\n",
        "            # Sadece \"ml\" değerlerini al\n",
        "            if magnitudeType == \"ML\":\n",
        "                ml_value = float(el[\"magnitude\"])\n",
        "\n",
        "                json_data = json.dumps({\n",
        "                    \"id\": i + 1,\n",
        "                    \"date\": el[\"eventDate\"],\n",
        "                    \"timestamp\": int(datetime.strptime(el[\"eventDate\"], \"%Y-%m-%dT%H:%M:%S\").timestamp()),\n",
        "                    \"latitude\": float(el[\"latitude\"]),\n",
        "                    \"longitude\": float(el[\"longitude\"]),\n",
        "                    \"depthAFAD\": float(el[\"depth\"]),\n",
        "                    \"size\": ml_value,\n",
        "                    \"location\": el[\"location\"],\n",
        "                    \"afadDetails\": {\n",
        "                        \"id\": el[\"id\"],\n",
        "                        \"refId\": el[\"refId\"]\n",
        "                    },\n",
        "                    \"attribute\": magnitudeType\n",
        "                }, sort_keys=False)\n",
        "\n",
        "                array.append(json.loads(json_data))\n",
        "\n",
        "        return array\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "Wlnr2al8B4D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getKandilliData():\n",
        "    try:\n",
        "        array = []\n",
        "        data = urlopen('http://www.koeri.boun.edu.tr/scripts/sondepremler.asp').read()\n",
        "        soup = BeautifulSoup(data, 'html.parser')\n",
        "        data = soup.find_all('pre')\n",
        "        data = str(data).strip().split('--------------')[2]\n",
        "\n",
        "        data = data.split('\\n')\n",
        "        data.pop(0)\n",
        "        data.pop()\n",
        "        data.pop()\n",
        "        for index in range(len(data)):\n",
        "            element = str(data[index].rstrip())\n",
        "            element = re.sub(r'\\s\\s\\s', ' ', element)\n",
        "            element = re.sub(r'\\s\\s\\s\\s', ' ', element)\n",
        "            element = re.sub(r'\\s\\s', ' ', element)\n",
        "            element = re.sub(r'\\s\\s', ' ', element)\n",
        "            Args = element.split(' ')\n",
        "            location = Args[8]+element.split(Args[8])[len(element.split(Args[8])) - 1].split('İlksel')[0].split('REVIZE')[0]\n",
        "\n",
        "            # Sadece \"ml\" değerlerini al\n",
        "            ml_value = float(Args[6].replace('-.-', '0'))\n",
        "\n",
        "            json_data = json.dumps({\n",
        "                \"id\": index+1,\n",
        "                \"date\": Args[0]+\" \"+Args[1],\n",
        "                \"timestamp\": int(datetime.strptime(Args[0]+\" \"+Args[1], \"%Y.%m.%d %H:%M:%S\").timestamp()),\n",
        "                \"latitude\": float(Args[2]),\n",
        "                \"longitude\": float(Args[3]),\n",
        "                \"depthKandilli\": float(Args[4]),\n",
        "                \"size\": ml_value,\n",
        "                \"location\": location.strip(),\n",
        "                \"attribute\": element.split(location)[1].split()[0]\n",
        "            }, sort_keys=False)\n",
        "\n",
        "            array.append(json.loads(json_data))\n",
        "        return array\n",
        "    except:\n",
        "        return []"
      ],
      "metadata": {
        "id": "7Zlpv5oYsP_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def job():\n",
        "    global afadData\n",
        "    global kandilliData\n",
        "    afadData = getAfadData()\n",
        "    kandilliData = getKandilliData()\n",
        "\n",
        "job()\n",
        "schedule.every(5).minutes.do(job)\n",
        "\n",
        "def threadFunction():\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(1)\n",
        "\n",
        "x = threading.Thread(target=threadFunction)\n",
        "x.start()"
      ],
      "metadata": {
        "id": "hbYh7LHG4ZW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZAy2jKj-sJp"
      },
      "outputs": [],
      "source": [
        "def filterByLocation(location, data):\n",
        "    return list(filter(lambda i: location.upper() in i['location'], data))\n",
        "\n",
        "\n",
        "def filterBySize(size, data):\n",
        "    return list(filter(lambda i: float(size) <= float(i['size']['ml']), data))\n",
        "\n",
        "\n",
        "def filterBySizeAndLocation(size, location, data):\n",
        "    return list(filter(lambda i: float(size) <= float(i['size']['ml']) and location.upper() in i['location'], data))\n",
        "\n",
        "\n",
        "def filterByTime(hour, data):\n",
        "    now = datetime.strptime(datetime.now().strftime(\n",
        "        \"%Y.%m.%d %H:%M:%S\"), '%Y.%m.%d %H:%M:%S')\n",
        "    return [record for record in data if (now - datetime.strptime(record['date'], \"%Y.%m.%d %H:%M:%S\")) <= timedelta(hours=int(hour))]\n",
        "\n",
        "\n",
        "def filterBySizeandtime(size, hour, data):\n",
        "    filtered_by_time = filterByTime(hour, data)\n",
        "    filtered_by_size = filterBySize(size, filtered_by_time)\n",
        "    return filtered_by_size\n",
        "\n",
        "\n",
        "def filterBySizeandtimeandlocation(size, hour, location, data):\n",
        "    filtered_by_time = filterByTime(hour, data)\n",
        "    filtered_by_size = filterBySize(size, filtered_by_time)\n",
        "    filtered_by_location = filterByLocation(location, filtered_by_size)\n",
        "    return filtered_by_location\n",
        "\n",
        "\n",
        "#bu kısım filtreleme kısmı hocanin verdigi lokasyona vs gore bu kısmı filtreleyebilirim fonk kullanarak"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pztLgdTDmNSi"
      },
      "outputs": [],
      "source": [
        "def saveDataToExcel(data, filename):\n",
        "    df = pd.DataFrame(data)\n",
        "    file_path = f'/content/drive/My Drive/{FOLDERNAME}{filename}'  # Update the file path\n",
        "    directory = f'/content/drive/My Drive/{FOLDERNAME}'\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    df.to_excel(file_path, index=False)\n",
        "\n",
        "while True:\n",
        "\n",
        "    # Save data to Excel files at each update\n",
        "    if afadData:\n",
        "        saveDataToExcel(afadData, 'afad_data.xlsx')\n",
        "    if kandilliData:\n",
        "        saveDataToExcel(kandilliData, 'kandilli_data.xlsx')\n",
        "    time.sleep(300)  # Wait for 5 minutes and check again"
      ]
    }
  ]
}